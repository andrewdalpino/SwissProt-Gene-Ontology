{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6fe696",
   "metadata": {},
   "source": [
    "In this notebook we'll load the SwissProt GO dataset and visualize the distribution of the GO terms. Then we'll assign stratum IDs to each sample based on their similarity to other samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_dataset_path = \"./dataset/all-filtered.jsonl\"\n",
    "mf_dataset_path = \"./dataset/mf-filtered.jsonl\"\n",
    "bp_dataset_path = \"./dataset/bp-filtered.jsonl\"\n",
    "cc_dataset_path = \"./dataset/cc-filtered.jsonl\"\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "top_k = 30\n",
    "\n",
    "all_term_counter = Counter()\n",
    "mf_term_counter = Counter()\n",
    "bp_term_counter = Counter()\n",
    "cc_term_counter = Counter()\n",
    "\n",
    "taxon_id_counter = Counter()\n",
    "\n",
    "for subset_name, dataset_path, term_counter in [\n",
    "    (\"All\", all_dataset_path, all_term_counter),\n",
    "    (\"Molecular Function\", mf_dataset_path, mf_term_counter),\n",
    "    (\"Biological Process\", bp_dataset_path, bp_term_counter),\n",
    "    (\"Cellular Component\", cc_dataset_path, cc_term_counter),\n",
    "]:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "    for record in dataset:\n",
    "        for term in record[\"terms\"]:\n",
    "            term_counter[term] += 1\n",
    "\n",
    "        taxon_id = record[\"taxon_id\"]\n",
    "\n",
    "        taxon_id_counter[taxon_id] += 1\n",
    "\n",
    "    term_counter = dict(sorted(term_counter.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    first_k = dict(islice(term_counter.items(), top_k))\n",
    "\n",
    "    plt.figure(figsize=(12, 5)) \n",
    "\n",
    "    plt.bar(first_k.keys(), first_k.values())\n",
    "\n",
    "    plt.title(f\"Top {top_k} {subset_name} Term Frequencies\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"GO Term ID\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0b691",
   "metadata": {},
   "source": [
    "Let's cluster the samples in the SwissProt GO dataset by their GO terms so we can use the cluster assignment to later do a stratified train/test split. We'll start by creating some embeddings for the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c7eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "min_explained_variance = 0.5\n",
    "\n",
    "all_terms_embeddings = {}\n",
    "mf_terms_embeddings = {}\n",
    "bp_terms_embeddings = {}\n",
    "cc_terms_embeddings = {}\n",
    "\n",
    "new_svd = partial(TruncatedSVD, n_iter=7, random_state=random_seed)\n",
    "\n",
    "for dataset_path, terms_embeddings, term_counter in [\n",
    "    (all_dataset_path, all_terms_embeddings, all_term_counter),\n",
    "    (mf_dataset_path, mf_terms_embeddings, mf_term_counter),\n",
    "    (bp_dataset_path, bp_terms_embeddings, bp_term_counter),\n",
    "    (cc_dataset_path, cc_terms_embeddings, cc_term_counter),\n",
    "]:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "    term_index_mapping = {term: index for index, term in enumerate(term_counter.keys())}\n",
    "\n",
    "    for record in dataset:\n",
    "        id = record[\"id\"]\n",
    "\n",
    "        template = np.zeros(len(term_index_mapping), dtype=np.int8)\n",
    "\n",
    "        for term in record[\"terms\"]:\n",
    "            if term in term_index_mapping:\n",
    "                index = term_index_mapping[term]\n",
    "\n",
    "                template[index] = 1\n",
    "\n",
    "        terms_embeddings[id] = template\n",
    "\n",
    "    best_dimensionality = 0\n",
    "    best_explained_variance = 0\n",
    "    best_model = None\n",
    "\n",
    "    for dimensionality in (32, 48, 64):\n",
    "        svd = new_svd(n_components=dimensionality)\n",
    "\n",
    "        x = np.stack(list(terms_embeddings.values()))\n",
    "        \n",
    "        svd.fit(x)\n",
    "        \n",
    "        explained_variance = np.sum(svd.explained_variance_ratio_)\n",
    "\n",
    "        if explained_variance > best_explained_variance:\n",
    "            best_dimensionality = dimensionality\n",
    "            best_explained_variance = explained_variance\n",
    "            best_model = svd\n",
    "\n",
    "        if explained_variance >= min_explained_variance:\n",
    "            break\n",
    "\n",
    "    print(f\"Best dimensionality: {best_dimensionality}\")\n",
    "    print(f\"Explained variance ratio: {best_explained_variance:.2f}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    z = best_model.transform(x)\n",
    "\n",
    "    for sequence_id, embedding in zip(terms_embeddings.keys(), z):\n",
    "        terms_embeddings[sequence_id] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df4f246",
   "metadata": {},
   "source": [
    "With the fresh embeddings, we'll cluster the sequences into strata of similar GO subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690dbfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "num_strata = 100\n",
    "\n",
    "all_stratum_ids = {}\n",
    "mf_stratum_ids = {}\n",
    "bp_stratum_ids = {}\n",
    "cc_stratum_ids = {}\n",
    "\n",
    "bp_stratum_counter = Counter()\n",
    "cc_stratum_counter = Counter()\n",
    "mf_stratum_counter = Counter()\n",
    "all_stratum_counter = Counter()\n",
    "\n",
    "new_kmeans = partial(KMeans, random_state=random_seed)\n",
    "\n",
    "for terms_embeddings, stratum_ids, stratum_counter in [\n",
    "    (all_terms_embeddings, all_stratum_ids, all_stratum_counter),\n",
    "    (mf_terms_embeddings, mf_stratum_ids, mf_stratum_counter),\n",
    "    (bp_terms_embeddings, bp_stratum_ids, bp_stratum_counter),\n",
    "    (cc_terms_embeddings, cc_stratum_ids, cc_stratum_counter),\n",
    "]:\n",
    "    kmeans = new_kmeans(n_clusters=num_strata)\n",
    "\n",
    "    x = np.stack(list(terms_embeddings.values()))\n",
    "\n",
    "    kmeans.fit(x)\n",
    "\n",
    "    print(f\"K-means steps: {kmeans.n_iter_}\")\n",
    "    print(f\"Inertia Loss: {kmeans.inertia_}\")\n",
    "\n",
    "    strata_ids = kmeans.predict(x)\n",
    "\n",
    "    for sequence_id, stratum_id in zip(terms_embeddings.keys(), strata_ids):\n",
    "        stratum_ids[sequence_id] = stratum_id\n",
    "\n",
    "        stratum_counter[stratum_id] += 1\n",
    "\n",
    "    print(f\"Number of unique strata: {len(stratum_counter):,}\")\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b93379",
   "metadata": {},
   "source": [
    "Let's plot the stratum counts to visualize how they are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, counter in [\n",
    "    (\"All\", all_stratum_counter),\n",
    "    (\"Biological Process\", bp_stratum_counter),\n",
    "    (\"Cellular Component\", cc_stratum_counter),\n",
    "    (\"Molecular Function\", mf_stratum_counter),\n",
    "]:\n",
    "    plt.figure(figsize=(12, 5)) \n",
    "\n",
    "    plt.bar(counter.keys(), counter.values())\n",
    "\n",
    "    plt.title(f\"{name} Stratum Frequencies\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Stratum ID\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86e311",
   "metadata": {},
   "source": [
    "Lastly, add the term embeddings and stratum IDs to the dataset and write to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "all_stratified_path = \"./dataset/all-stratified.jsonl\"\n",
    "mf_stratified_path = \"./dataset/mf-stratified.jsonl\"\n",
    "bp_stratified_path = \"./dataset/bp-stratified.jsonl\"\n",
    "cc_stratified_path = \"./dataset/cc-stratified.jsonl\"\n",
    "\n",
    "for dataset_path, stratified_path, terms_embeddings, stratum_ids in [\n",
    "    (all_dataset_path, all_stratified_path, all_terms_embeddings, all_stratum_ids),\n",
    "    (mf_dataset_path, mf_stratified_path, mf_terms_embeddings, mf_stratum_ids),\n",
    "    (bp_dataset_path, bp_stratified_path, bp_terms_embeddings, bp_stratum_ids),\n",
    "    (cc_dataset_path, cc_stratified_path, cc_terms_embeddings, cc_stratum_ids),\n",
    "]:\n",
    "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "    for record in dataset:\n",
    "        id = record[\"id\"]\n",
    "\n",
    "        record[\"terms_embedding\"] = terms_embeddings[id].tolist()\n",
    "        record[\"stratum_id\"] = stratum_ids[id]\n",
    "\n",
    "    with open(stratified_path, \"w\") as file:\n",
    "        for record in dataset:\n",
    "            file.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "    print(f\"Dataset saved to {stratified_path}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
