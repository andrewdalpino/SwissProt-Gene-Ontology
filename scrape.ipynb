{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a89ff6",
   "metadata": {},
   "source": [
    "Let's loop through the sequences of the SwissProt dataset and scrape their GO term annotations from the UniProt API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720979b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record: #243,543, ID: Q4QR85, Length: 342, Num Terms: 19\n",
      "Record: #243,544, ID: Q6NUD0, Length: 333, Num Terms: 4\n",
      "Record: #243,545, ID: D4AQG5, Length: 633, Num Terms: 4\n",
      "Record: #243,546, ID: Q6WIH6, Length: 633, Num Terms: 4\n",
      "Record: #243,547, ID: E4UVK2, Length: 642, Num Terms: 4\n",
      "Record: #243,548, ID: C5FX29, Length: 632, Num Terms: 4\n",
      "Record: #243,549, ID: Q6WIH7, Length: 632, Num Terms: 4\n",
      "Record: #243,550, ID: C5NZY5, Length: 356, Num Terms: 4\n",
      "Record: #243,551, ID: Q8NIH1, Length: 633, Num Terms: 4\n",
      "Record: #243,552, ID: A5YCB9, Length: 608, Num Terms: 4\n",
      "Record: #243,553, ID: D4DIT1, Length: 633, Num Terms: 4\n",
      "Record: #243,554, ID: D4ALW9, Length: 271, Num Terms: 4\n",
      "Record: #243,555, ID: E4V4I7, Length: 271, Num Terms: 4\n",
      "Record: #243,556, ID: C5FQJ4, Length: 270, Num Terms: 4\n",
      "Record: #243,557, ID: C5PE18, Length: 355, Num Terms: 4\n"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "from os import path\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "fasta_path = \"./dataset/uniprot_sprot.fasta\"\n",
    "\n",
    "start_offset = 243543\n",
    "\n",
    "mf_dataset_path = \"./dataset/mf.jsonl\"\n",
    "bp_dataset_path = \"./dataset/bp.jsonl\"\n",
    "cc_dataset_path = \"./dataset/cc.jsonl\"\n",
    "all_dataset_path = \"./dataset/all.jsonl\"\n",
    "\n",
    "params = {\n",
    "    \"fields\": [\n",
    "        \" go_p\",\n",
    "        \" go_c\",\n",
    "        \" go_f\",\n",
    "        \" go_id\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "base_url = \"https://rest.uniprot.org/uniprotkb\"\n",
    "\n",
    "with open(mf_dataset_path, \"a\") as mf_dataset_file, \\\n",
    "    open(bp_dataset_path, \"a\") as bp_dataset_file, \\\n",
    "    open(cc_dataset_path, \"a\") as cc_dataset_file, \\\n",
    "    open(all_dataset_path, \"a\") as all_dataset_file, \\\n",
    "    open(fasta_path, \"r\") as fasta_file:\n",
    "\n",
    "    for index, record in enumerate(SeqIO.parse(fasta_file, \"fasta\"), start=1):\n",
    "        if index < start_offset:\n",
    "            continue\n",
    "\n",
    "        sequence_id = record.id.split(\"|\")[1]\n",
    "        taxon_id = record.description.split(\"OX=\", 1)[1].split(\" \")[0]\n",
    "        sequence = str(record.seq)\n",
    "\n",
    "        url = path.join(base_url, sequence_id)\n",
    "\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        while response.status_code == 503:\n",
    "            retry_after = int(response.headers.get(\"Retry-After\", 5))\n",
    "\n",
    "            print(f\"Rate limit exceeded. Retrying in {retry_after} seconds ...\")\n",
    "\n",
    "            sleep(retry_after)\n",
    "\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching data for {sequence_id}: {response.status_code}\")\n",
    "\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        bp_terms = []\n",
    "        cc_terms = []\n",
    "        mf_terms = []\n",
    "        all_terms = []\n",
    "\n",
    "        if \"uniProtKBCrossReferences\" not in data:\n",
    "            continue\n",
    "\n",
    "        for go_object in data[\"uniProtKBCrossReferences\"]:\n",
    "            if \"database\" not in go_object:\n",
    "                continue\n",
    "\n",
    "            if go_object[\"database\"] == \"GO\":\n",
    "                id = go_object[\"id\"]\n",
    "\n",
    "                aspect = \"UNK\"\n",
    "                evidence_code = \"UNK\"\n",
    "\n",
    "                for property in go_object[\"properties\"]:\n",
    "                    if property[\"key\"] == \"GoTerm\":\n",
    "                        value = property[\"value\"]\n",
    "\n",
    "                        aspect, _ = value.split(\":\", 1)\n",
    "\n",
    "                    if property[\"key\"] == \"GoEvidenceType\":\n",
    "                        evidence_code = property[\"value\"].split(\":\", 1)[0]\n",
    "\n",
    "                go_term = {\n",
    "                    \"id\": id,\n",
    "                    \"evidence_code\": evidence_code,\n",
    "                }\n",
    "\n",
    "                match(aspect.upper()):\n",
    "                    case \"P\":\n",
    "                        bp_terms.append(go_term)\n",
    "                    case \"C\":\n",
    "                        cc_terms.append(go_term)\n",
    "                    case \"F\":\n",
    "                        mf_terms.append(go_term)\n",
    "\n",
    "                all_terms.append(go_term)\n",
    "\n",
    "        if len(mf_terms) > 0:\n",
    "            mf_dataset_file.write(json.dumps({\n",
    "                \"id\": sequence_id,\n",
    "                \"sequence\": sequence,\n",
    "                \"terms\": list(mf_terms),\n",
    "                \"taxon_id\": str(taxon_id),\n",
    "            }) + \"\\n\")\n",
    "\n",
    "        if len(bp_terms) > 0:\n",
    "            bp_dataset_file.write(json.dumps({\n",
    "                \"id\": sequence_id,\n",
    "                \"sequence\": sequence,\n",
    "                \"terms\": list(bp_terms),\n",
    "                \"taxon_id\": str(taxon_id),\n",
    "            }) + \"\\n\")\n",
    "\n",
    "        if len(cc_terms) > 0:\n",
    "            cc_dataset_file.write(json.dumps({\n",
    "                \"id\": sequence_id,\n",
    "                \"sequence\": sequence,\n",
    "                \"terms\": list(cc_terms),\n",
    "                \"taxon_id\": str(taxon_id),\n",
    "            }) + \"\\n\")\n",
    "\n",
    "        if len(all_terms) > 0:\n",
    "            all_dataset_file.write(json.dumps({\n",
    "                \"id\": sequence_id,\n",
    "                \"sequence\": sequence,\n",
    "                \"terms\": list(all_terms),\n",
    "                \"taxon_id\": str(taxon_id),\n",
    "            }) + \"\\n\")\n",
    "\n",
    "        print(\n",
    "            f\"Record: #{index:,}, ID: {sequence_id}, \"\n",
    "            f\"Length: {len(sequence)}, GO Terms: {len(all_terms):,}\"\n",
    "        )\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
